// scripts/test-crawler.ts

import { WebCrawler } from '../src/lib/crawler'
import { TextChunker } from '../src/lib/text-chunker'

async function testCrawler() {
  console.log('üß™ Testing Web Crawler...\n')

  const crawler = new WebCrawler({
    maxDepth: 1,
    maxPages: 5,
    timeout: 10000,
  })

  const chunker = new TextChunker({
    chunkSize: 512,
    chunkOverlap: 50,
  })

  // Test URL (mo≈æe≈° zameniti sa pravim ELAB URL-om)
  const testUrl = 'https://elab.fon.bg.ac.rs'

  try {
    const documents = await crawler.crawl(testUrl)

    console.log('\nüìä Crawl Statistics:')
    console.log(crawler.getStats())

    console.log('\nüìÑ Sample Document:')
    if (documents.length > 0) {
      const doc = documents[0]
      console.log('Title:', doc.title)
      console.log('URL:', doc.url)
      console.log('Content length:', doc.content.length)
      console.log('Content preview:', doc.content.slice(0, 200) + '...')

      console.log('\nüî™ Creating chunks...')
      const chunks = chunker.createChunks(doc.content)
      
      console.log('\nüìä Chunk Statistics:')
      console.log(chunker.getStats(chunks))

      console.log('\nüì¶ Sample Chunk:')
      if (chunks.length > 0) {
        console.log('Chunk #0:', chunks[0].content.slice(0, 200) + '...')
      }
    }
  } catch (error) {
    console.error('‚ùå Test failed:', error)
  }
}

testCrawler()